{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the raw mooring data\n",
    "\n",
    "Contents:\n",
    "* <a href=#raw>Raw data reprocessing.</a>\n",
    "* <a href=#corrected>Interpolated data processing.</a>\n",
    "* <a href=#ADCP>ADCP processing.</a>\n",
    "* <a href=#VMP>VMP processing.</a>\n",
    "\n",
    "Import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import gsw\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import scipy.integrate as igr\n",
    "import scipy.interpolate as itpl\n",
    "import scipy.io as io\n",
    "import scipy.signal as sig\n",
    "import seawater\n",
    "import xarray as xr\n",
    "from matplotlib import path\n",
    "import munch\n",
    "\n",
    "import load_data\n",
    "import moorings as moo\n",
    "import utils\n",
    "from oceans.sw_extras import gamma_GP_from_SP_pt\n",
    "\n",
    "# Data directory\n",
    "data_in = os.path.expanduser(\"../data\")\n",
    "data_out = data_in\n",
    "\n",
    "\n",
    "def esum(ea, eb):\n",
    "    return np.sqrt(ea ** 2 + eb ** 2)\n",
    "\n",
    "\n",
    "def emult(a, b, ea, eb):\n",
    "    return np.abs(a * b) * np.sqrt((ea / a) ** 2 + (eb / b) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"raw\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process raw data into a more convenient format\n",
    "\n",
    "Parameters for raw processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected levels.\n",
    "# heights = [-540., -1250., -2100., -3500.]\n",
    "# Filter cut off (hours)\n",
    "tc_hrs = 40.0\n",
    "# Start of time series (matlab datetime)\n",
    "t_start = 734494.0\n",
    "# Length of time series\n",
    "max_len = N_data = 42048\n",
    "# Data file\n",
    "raw_data_file = \"moorings.mat\"\n",
    "# Index where NaNs start in u and v data from SW mooring\n",
    "sw_vel_nans = 14027\n",
    "# Sampling period (minutes)\n",
    "dt_min = 15.0\n",
    "# Window length for wave stress quantities and mesoscale strain quantities.\n",
    "nperseg = 2 ** 9\n",
    "# Spectra parameters\n",
    "window = \"hanning\"\n",
    "detrend = \"constant\"\n",
    "# Extrapolation/interpolation limit above which data will be removed.\n",
    "dzlim = 100.0\n",
    "# Integration of spectra parameters. These multiple N and f respectively to set\n",
    "# the integration limits.\n",
    "fhi = 1.0\n",
    "flo = 1.0\n",
    "flov = 1.0  # When integrating spectra involved in vertical fluxes, get rid of\n",
    "# the near inertial portion.\n",
    "# When bandpass filtering windowed data use these params multiplied by f and N\n",
    "filtlo = 0.9  # times f\n",
    "filthi = 1.1  # times N\n",
    "\n",
    "# Interpolation distance that raises flag (m)\n",
    "zimax = 100.0\n",
    "\n",
    "dt_sec = dt_min * 60.0  # Sample period in seconds.\n",
    "dt_day = dt_sec / 86400.0  # Sample period in days.\n",
    "N_per_day = int(1.0 / dt_day)  # Samples per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "############### PROCESS RAW DATA #########################################"
   },
   "outputs": [],
   "source": [
    "print(\"RAW DATA\")\n",
    "###############################################################################\n",
    "# Load w data for cc mooring and chop from text files. I checked and all the\n",
    "# data has the same start date and the same length\n",
    "print(\"Loading vertical velocity data from text files.\")\n",
    "nortek_files = glob.glob(os.path.join(data_in, \"cc_1_*.txt\"))\n",
    "\n",
    "depth = []\n",
    "for file in nortek_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "        depth.append(int(content[3].split(\"=\")[1].split()[0]))\n",
    "\n",
    "idxs = np.argsort(depth)\n",
    "\n",
    "w = np.empty((42573, 12))\n",
    "datenum = np.empty((42573, 12))\n",
    "\n",
    "for i in idxs:\n",
    "    YY, MM, DD, hh, W = np.genfromtxt(\n",
    "        nortek_files[i], skip_header=12, usecols=(0, 1, 2, 3, 8), unpack=True\n",
    "    )\n",
    "    YY = YY.astype(int)\n",
    "    MM = MM.astype(int)\n",
    "    DD = DD.astype(int)\n",
    "    mm = (60 * (hh % 1)).astype(int)\n",
    "    hh = np.floor(hh).astype(int)\n",
    "    w[:, i] = W / 100\n",
    "    dates = []\n",
    "    for j in range(len(YY)):\n",
    "        dates.append(datetime.datetime(YY[j], MM[j], DD[j], hh[j], mm[j]))\n",
    "    dates = np.asarray(dates)\n",
    "    datenum[:, i] = utils.datetime_to_datenum(dates)\n",
    "\n",
    "idx_start = np.searchsorted(datenum[:, 0], t_start)\n",
    "w = w[idx_start : idx_start + max_len]\n",
    "\n",
    "# Start prepping raw data from the mat file.\n",
    "print(\"Loading raw data file.\")\n",
    "data_path = os.path.join(data_in, raw_data_file)\n",
    "ds = utils.loadmat(data_path)\n",
    "\n",
    "cc = ds.pop(\"c\")\n",
    "nw = ds.pop(\"nw\")\n",
    "ne = ds.pop(\"ne\")\n",
    "se = ds.pop(\"se\")\n",
    "sw = ds.pop(\"sw\")\n",
    "\n",
    "cc[\"id\"] = \"cc\"\n",
    "nw[\"id\"] = \"nw\"\n",
    "ne[\"id\"] = \"ne\"\n",
    "se[\"id\"] = \"se\"\n",
    "sw[\"id\"] = \"sw\"\n",
    "\n",
    "moorings = [cc, nw, ne, se, sw]\n",
    "\n",
    "# Useful information\n",
    "dt_min = 15.0  # Sample period in minutes.\n",
    "dt_sec = dt_min * 60.0  # Sample period in seconds.\n",
    "dt_day = dt_sec / 86400.0  # Sample period in days.\n",
    "\n",
    "print(\"Chopping time series.\")\n",
    "for m in moorings:\n",
    "    m[\"idx_start\"] = np.searchsorted(m[\"Dates\"], t_start)\n",
    "\n",
    "for m in moorings:\n",
    "    m[\"N_data\"] = max_len\n",
    "    m[\"idx_end\"] = m[\"idx_start\"] + max_len\n",
    "\n",
    "# Chop data to start and end dates.\n",
    "varl = [\"Dates\", \"Temp\", \"Sal\", \"u\", \"v\", \"Pres\"]\n",
    "for m in moorings:\n",
    "    for var in varl:\n",
    "        m[var] = m[var][m[\"idx_start\"] : m[\"idx_end\"], ...]\n",
    "\n",
    "\n",
    "print(\"Renaming variables.\")\n",
    "print(\"Interpolating negative pressures.\")\n",
    "for m in moorings:\n",
    "    __, N_levels = m[\"Pres\"].shape\n",
    "    m[\"N_levels\"] = N_levels\n",
    "\n",
    "    # Tile time and pressure\n",
    "    m[\"t\"] = np.tile(m.pop(\"Dates\")[:, np.newaxis], (1, N_levels))\n",
    "\n",
    "    # Fix negative pressures by interpolating nearby data.\n",
    "    fix = m[\"Pres\"] < 0.0\n",
    "    if fix.any():\n",
    "        levs = np.argwhere(np.any(fix, axis=0))[0]\n",
    "        for lev in levs:\n",
    "            x = m[\"t\"][fix[:, lev], lev]\n",
    "            xp = m[\"t\"][~fix[:, lev], lev]\n",
    "            fp = m[\"Pres\"][~fix[:, lev], lev]\n",
    "            m[\"Pres\"][fix[:, lev], lev] = np.interp(x, xp, fp)\n",
    "\n",
    "    # Rename variables\n",
    "    m[\"P\"] = m.pop(\"Pres\")\n",
    "    m[\"u\"] = m[\"u\"] / 100.0\n",
    "    m[\"v\"] = m[\"v\"] / 100.0\n",
    "    m[\"spd\"] = np.sqrt(m[\"u\"] ** 2 + m[\"v\"] ** 2)\n",
    "    m[\"angle\"] = np.angle(m[\"u\"] + 1j * m[\"v\"])\n",
    "    m[\"Sal\"][(m[\"Sal\"] < 33.5) | (m[\"Sal\"] > 34.9)] = np.nan\n",
    "    m[\"S\"] = m.pop(\"Sal\")\n",
    "    m[\"Temp\"][m[\"Temp\"] < -2.0] = np.nan\n",
    "    m[\"T\"] = m.pop(\"Temp\")\n",
    "\n",
    "    # Dimensional quantities.\n",
    "    m[\"f\"] = gsw.f(m[\"lat\"])\n",
    "    m[\"ll\"] = np.array([m[\"lon\"], m[\"lat\"]])\n",
    "    m[\"z\"] = gsw.z_from_p(m[\"P\"], m[\"lat\"])\n",
    "\n",
    "    # Estimate thermodynamic quantities.\n",
    "    m[\"SA\"] = gsw.SA_from_SP(m[\"S\"], m[\"P\"], m[\"lon\"], m[\"lat\"])\n",
    "    m[\"CT\"] = gsw.CT_from_t(m[\"SA\"], m[\"T\"], m[\"P\"])\n",
    "    # specvol_anom = gsw.specvol_anom(m['SA'], m['CT'], m['P'])\n",
    "    # m['sva'] = specvol_anom\n",
    "\n",
    "cc[\"wr\"] = w\n",
    "\n",
    "print(\"Calculating thermodynamics.\")\n",
    "print(\"Excluding bad data using T-S funnel.\")\n",
    "# Chuck out data outside of TS funnel sensible range.\n",
    "funnel = np.genfromtxt(\"funnel.txt\")\n",
    "\n",
    "for m in moorings:\n",
    "    S = m[\"SA\"].flatten()\n",
    "    T = m[\"CT\"].flatten()\n",
    "    p = path.Path(funnel)\n",
    "    in_funnel = p.contains_points(np.vstack((S, T)).T)\n",
    "    fix = np.reshape(~in_funnel, m[\"SA\"].shape)\n",
    "    m[\"in_funnel\"] = ~fix\n",
    "\n",
    "    varl = [\"S\"]\n",
    "    if fix.any():\n",
    "        levs = np.squeeze(np.argwhere(np.any(fix, axis=0)))\n",
    "        for lev in levs:\n",
    "            x = m[\"t\"][fix[:, lev], lev]\n",
    "            xp = m[\"t\"][~fix[:, lev], lev]\n",
    "            for var in varl:\n",
    "                fp = m[var][~fix[:, lev], lev]\n",
    "                m[var][fix[:, lev], lev] = np.interp(x, xp, fp)\n",
    "\n",
    "    # Re-estimate thermodynamic quantities.\n",
    "    m[\"SA\"] = gsw.SA_from_SP(m[\"S\"], m[\"P\"], m[\"lon\"], m[\"lat\"])\n",
    "    m[\"CT\"] = gsw.CT_from_t(m[\"SA\"], m[\"T\"], m[\"P\"])\n",
    "\n",
    "print(\"Calculating neutral density.\")\n",
    "# Estimate the neutral density\n",
    "for m in moorings:\n",
    "    # Compute potential temperature using the 1983 UNESCO EOS.\n",
    "    m[\"PT0\"] = seawater.ptmp(m[\"S\"], m[\"T\"], m[\"P\"])\n",
    "    # Flatten variables for analysis.\n",
    "    lons = m[\"lon\"] * np.ones_like(m[\"P\"])\n",
    "    lats = m[\"lat\"] * np.ones_like(m[\"P\"])\n",
    "    S_ = m[\"S\"].flatten()\n",
    "    T_ = m[\"PT0\"].flatten()\n",
    "    P_ = m[\"P\"].flatten()\n",
    "    LO_ = lons.flatten()\n",
    "    LA_ = lats.flatten()\n",
    "    gamman = gamma_GP_from_SP_pt(S_, T_, P_, LO_, LA_)\n",
    "    m[\"gamman\"] = np.reshape(gamman, m[\"P\"].shape) + 1000.0\n",
    "\n",
    "print(\"Calculating slice gradients at C.\")\n",
    "# Want gradient of density/vel to be local, no large central differences.\n",
    "slices = [slice(0, 4), slice(4, 6), slice(6, 10), slice(10, 12)]\n",
    "cc[\"dgdz\"] = np.empty((cc[\"N_data\"], cc[\"N_levels\"]))\n",
    "cc[\"dTdz\"] = np.empty((cc[\"N_data\"], cc[\"N_levels\"]))\n",
    "cc[\"dudz\"] = np.empty((cc[\"N_data\"], cc[\"N_levels\"]))\n",
    "cc[\"dvdz\"] = np.empty((cc[\"N_data\"], cc[\"N_levels\"]))\n",
    "for sl in slices:\n",
    "    z = cc[\"z\"][:, sl]\n",
    "    g = cc[\"gamman\"][:, sl]\n",
    "    T = cc[\"T\"][:, sl]\n",
    "    u = cc[\"u\"][:, sl]\n",
    "    v = cc[\"v\"][:, sl]\n",
    "    cc[\"dgdz\"][:, sl] = np.gradient(g, axis=1) / np.gradient(z, axis=1)\n",
    "    cc[\"dTdz\"][:, sl] = np.gradient(T, axis=1) / np.gradient(z, axis=1)\n",
    "    cc[\"dudz\"][:, sl] = np.gradient(u, axis=1) / np.gradient(z, axis=1)\n",
    "    cc[\"dvdz\"][:, sl] = np.gradient(v, axis=1) / np.gradient(z, axis=1)\n",
    "\n",
    "print(\"Filtering data.\")\n",
    "# Low pass filter data.\n",
    "tc = tc_hrs * 60.0 * 60.0\n",
    "fc = 1.0 / tc  # Cut off frequency.\n",
    "normal_cutoff = fc * dt_sec * 2.0  # Nyquist frequency is half 1/dt.\n",
    "b, a = sig.butter(4, normal_cutoff, btype=\"lowpass\")\n",
    "\n",
    "varl = [\n",
    "    \"z\",\n",
    "    \"P\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"wr\",\n",
    "    \"SA\",\n",
    "    \"CT\",\n",
    "    \"gamman\",\n",
    "    \"dgdz\",\n",
    "    \"dTdz\",\n",
    "    \"dudz\",\n",
    "    \"dvdz\",\n",
    "]  # sva\n",
    "for m in moorings:\n",
    "    for var in varl:\n",
    "        try:\n",
    "            data = m[var].copy()\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        m[var + \"_m\"] = np.nanmean(data, axis=0)\n",
    "\n",
    "        # For the purpose of filtering set fill with 0 rather than nan (SW)\n",
    "        nans = np.isnan(data)\n",
    "        if nans.any():\n",
    "            data[nans] = 0.0\n",
    "\n",
    "        datalo = sig.filtfilt(b, a, data, axis=0)\n",
    "\n",
    "        # Then put nans back...\n",
    "        if nans.any():\n",
    "            datalo[nans] = np.nan\n",
    "\n",
    "        namelo = var + \"_lo\"\n",
    "        m[namelo] = datalo\n",
    "        namehi = var + \"_hi\"\n",
    "        m[namehi] = m[var] - m[namelo]\n",
    "\n",
    "    m[\"spd_lo\"] = np.sqrt(m[\"u_lo\"] ** 2 + m[\"v_lo\"] ** 2)\n",
    "    m[\"angle_lo\"] = ma.angle(m[\"u_lo\"] + 1j * m[\"v_lo\"])\n",
    "    m[\"spd_hi\"] = np.sqrt(m[\"u_hi\"] ** 2 + m[\"v_hi\"] ** 2)\n",
    "    m[\"angle_hi\"] = ma.angle(m[\"u_hi\"] + 1j * m[\"v_hi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "##################### SAVE RAW DATA ######################################"
   },
   "outputs": [],
   "source": [
    "io.savemat(os.path.join(data_out, \"C_raw.mat\"), cc)\n",
    "io.savemat(os.path.join(data_out, \"NW_raw.mat\"), nw)\n",
    "io.savemat(os.path.join(data_out, \"NE_raw.mat\"), ne)\n",
    "io.savemat(os.path.join(data_out, \"SE_raw.mat\"), se)\n",
    "io.savemat(os.path.join(data_out, \"SW_raw.mat\"), sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create virtual mooring 'raw'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VIRTUAL MOORING\")\n",
    "print(\"Determine maximum knockdown as a function of z.\")\n",
    "\n",
    "zms = np.hstack([m[\"z\"].max(axis=0) for m in moorings if \"se\" not in m[\"id\"]])\n",
    "Dzs = np.hstack(\n",
    "    [m[\"z\"].min(axis=0) - m[\"z\"].max(axis=0) for m in moorings if \"se\" not in m[\"id\"]]\n",
    ")\n",
    "\n",
    "zmax_pfit = np.polyfit(zms, Dzs, 2)  # Second order polynomial for max knockdown\n",
    "\n",
    "np.save(\n",
    "    os.path.join(data_out, \"zmax_pfit\"), np.polyfit(zms, Dzs, 2), allow_pickle=False\n",
    ")\n",
    "\n",
    "# Define the knockdown model:\n",
    "def zmodel(u, zmax, zmax_pfit):\n",
    "    return zmax + np.polyval(zmax_pfit, zmax) * u ** 3\n",
    "\n",
    "\n",
    "print(\"Load model data.\")\n",
    "mluv = xr.load_dataset(\"../data/mooring_locations_uv1.nc\")\n",
    "mluv = mluv.isel(\n",
    "    t=slice(0, np.argwhere(mluv.u[:, 0, 0].data == 0)[0][0])\n",
    ")  # Get rid of end zeros...\n",
    "mluv = mluv.assign_coords(lon=mluv.lon)\n",
    "mluv = mluv.assign_coords(id=[\"cc\", \"nw\", \"ne\", \"se\", \"sw\"])\n",
    "mluv[\"spd\"] = (mluv.u ** 2 + mluv.v ** 2) ** 0.5\n",
    "\n",
    "\n",
    "print(\"Create virtual mooring 'raw' dataset.\")\n",
    "savedict = {\n",
    "    \"cc\": {\"id\": \"cc\"},\n",
    "    \"nw\": {\"id\": \"nw\"},\n",
    "    \"ne\": {\"id\": \"ne\"},\n",
    "    \"se\": {\"id\": \"se\"},\n",
    "    \"sw\": {\"id\": \"sw\"},\n",
    "}\n",
    "mids = [\"cc\", \"nw\", \"ne\", \"se\", \"sw\"]\n",
    "\n",
    "\n",
    "def nearidx(a, v):\n",
    "    return np.argmin(np.abs(np.asarray(a) - v))\n",
    "\n",
    "\n",
    "for idx, mid in enumerate(mids):\n",
    "    savedict[mid][\"lon\"] = mluv.lon[idx].data\n",
    "    savedict[mid][\"lat\"] = mluv.lat[idx].data\n",
    "\n",
    "    izs = []\n",
    "    for i in range(moorings[idx][\"N_levels\"]):\n",
    "        izs.append(nearidx(mluv.z, moorings[idx][\"z\"][:, i].max()))\n",
    "\n",
    "    spdm = mluv.spd.isel(z=izs, index=idx).mean(dim=\"z\")\n",
    "    spdn = spdm / spdm.max()\n",
    "    zmax = mluv.z[izs]\n",
    "\n",
    "    zk = zmodel(spdn.data[:, np.newaxis], zmax.data[np.newaxis, :], zmax_pfit)\n",
    "    savedict[mid][\"z\"] = zk\n",
    "    savedict[mid][\"t\"] = np.tile(\n",
    "        mluv.t.data[:, np.newaxis], (1, moorings[idx][\"N_levels\"])\n",
    "    )\n",
    "\n",
    "    fu = itpl.RectBivariateSpline(mluv.t.data, -mluv.z.data, mluv.u[..., idx].data)\n",
    "    fv = itpl.RectBivariateSpline(mluv.t.data, -mluv.z.data, mluv.v[..., idx].data)\n",
    "\n",
    "    uk = fu(mluv.t.data[:, np.newaxis], -zk, grid=False)\n",
    "    vk = fv(mluv.t.data[:, np.newaxis], -zk, grid=False)\n",
    "    savedict[mid][\"u\"] = uk\n",
    "    savedict[mid][\"v\"] = vk\n",
    "\n",
    "io.savemat(\"../data/virtual_mooring_raw.mat\", savedict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create virtual mooring 'interpolated'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected levels.\n",
    "# heights = [-540., -1250., -2100., -3500.]\n",
    "# Filter cut off (hours)\n",
    "tc_hrs = 40.0\n",
    "# Start of time series (matlab datetime)\n",
    "# t_start = 734494.0\n",
    "# Length of time series\n",
    "# max_len = N_data = 42048\n",
    "# Sampling period (minutes)\n",
    "dt_min = 60.0\n",
    "dt_sec = dt_min * 60.0  # Sample period in seconds.\n",
    "dt_day = dt_sec / 86400.0  # Sample period in days.\n",
    "N_per_day = int(1.0 / dt_day)  # Samples per day.\n",
    "# Window length for wave stress quantities and mesoscale strain quantities.\n",
    "nperseg = 2 ** 7\n",
    "# Spectra parameters\n",
    "window = \"hanning\"\n",
    "detrend = \"constant\"\n",
    "# Extrapolation/interpolation limit above which data will be removed.\n",
    "dzlim = 100.0\n",
    "# Integration of spectra parameters. These multiple N and f respectively to set\n",
    "# the integration limits.\n",
    "fhi = 1.0\n",
    "flo = 1.0\n",
    "flov = 1.0  # When integrating spectra involved in vertical fluxes, get rid of\n",
    "# the near inertial portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moorings = utils.loadmat(\"../data/virtual_mooring_raw.mat\")\n",
    "\n",
    "cc = moorings.pop(\"cc\")\n",
    "nw = moorings.pop(\"nw\")\n",
    "ne = moorings.pop(\"ne\")\n",
    "se = moorings.pop(\"se\")\n",
    "sw = moorings.pop(\"sw\")\n",
    "moorings = [cc, nw, ne, se, sw]\n",
    "\n",
    "N_data = cc[\"t\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial fits first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**Generating corrected data**\")\n",
    "# Generate corrected moorings\n",
    "z = np.concatenate([m[\"z\"].flatten() for m in moorings])\n",
    "u = np.concatenate([m[\"u\"].flatten() for m in moorings])\n",
    "v = np.concatenate([m[\"v\"].flatten() for m in moorings])\n",
    "\n",
    "print(\"Calculating polynomial coefficients.\")\n",
    "pzu = np.polyfit(z, u, 2)\n",
    "pzv = np.polyfit(z, v, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional height in m to add to interpolation height.\n",
    "hoffset = [-25.0, 50.0, -50.0, 100.0]\n",
    "\n",
    "pi2 = np.pi * 2.0\n",
    "nfft = nperseg\n",
    "levis = [(0, 1, 2, 3), (4, 5), (6, 7, 8, 9), (10, 11)]\n",
    "Nclevels = len(levis)\n",
    "spec_kwargs = {\n",
    "    \"fs\": 1.0 / dt_sec,\n",
    "    \"window\": window,\n",
    "    \"nperseg\": nperseg,\n",
    "    \"nfft\": nfft,\n",
    "    \"detrend\": detrend,\n",
    "    \"axis\": 0,\n",
    "}\n",
    "\n",
    "idx1 = np.arange(nperseg, N_data, nperseg // 2)  # Window end index\n",
    "idx0 = idx1 - nperseg  # Window start index\n",
    "N_windows = len(idx0)\n",
    "\n",
    "# Initialise the place holder dictionaries.\n",
    "c12w = {\"N_levels\": 12}  # Dictionary for raw, windowed data from central mooring\n",
    "c4w = {\"N_levels\": Nclevels}  # Dictionary for processed, windowed data\n",
    "c4 = {\"N_levels\": Nclevels}  # Dictionary for processed data\n",
    "# Dictionaries for raw, windowed data from outer moorings\n",
    "nw5w, ne5w, se5w, sw5w = {\"id\": \"nw\"}, {\"id\": \"ne\"}, {\"id\": \"se\"}, {\"id\": \"sw\"}\n",
    "moorings5w = [nw5w, ne5w, se5w, sw5w]\n",
    "# Dictionaries for processed, windowed data from outer moorings\n",
    "nw4w, ne4w, se4w, sw4w = {\"id\": \"nw\"}, {\"id\": \"ne\"}, {\"id\": \"se\"}, {\"id\": \"sw\"}\n",
    "moorings4w = [nw4w, ne4w, se4w, sw4w]\n",
    "\n",
    "# Initialised the arrays of windowed data\n",
    "varr = [\"t\", \"z\", \"u\", \"v\"]\n",
    "for var in varr:\n",
    "    c12w[var] = np.zeros((nperseg, N_windows, 12))\n",
    "\n",
    "var4 = [\n",
    "    \"t\",\n",
    "    \"z\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"dudx\",\n",
    "    \"dvdx\",\n",
    "    \"dudy\",\n",
    "    \"dvdy\",\n",
    "    \"dudz\",\n",
    "    \"dvdz\",\n",
    "    \"nstrain\",\n",
    "    \"sstrain\",\n",
    "    \"vort\",\n",
    "    \"div\",\n",
    "]\n",
    "for var in var4:\n",
    "    c4w[var] = np.zeros((nperseg, N_windows, Nclevels))\n",
    "\n",
    "for var in var4:\n",
    "    c4[var] = np.zeros((N_windows, Nclevels))\n",
    "\n",
    "# Initialised the arrays of windowed data for outer mooring\n",
    "varro = [\"z\", \"u\", \"v\"]\n",
    "for var in varro:\n",
    "    for m5w in moorings5w:\n",
    "        m5w[var] = np.zeros((nperseg, N_windows, 5))\n",
    "\n",
    "var4o = [\"z\", \"u\", \"v\"]\n",
    "for var in var4o:\n",
    "    for m4w in moorings4w:\n",
    "        m4w[var] = np.zeros((nperseg, N_windows, Nclevels))\n",
    "\n",
    "# for var in var4o:\n",
    "#     for m4 in moorings4:\n",
    "#         m4[var] = np.zeros((N_windows, 4))\n",
    "\n",
    "# Window the raw data.\n",
    "for i in range(N_windows):\n",
    "    idx = idx0[i]\n",
    "    for var in varr:\n",
    "        c12w[var][:, i, :] = cc[var][idx : idx + nperseg, :]\n",
    "\n",
    "for i in range(N_windows):\n",
    "    idx = idx0[i]\n",
    "    for var in varro:\n",
    "        for m5w, m in zip(moorings5w, moorings[1:]):\n",
    "            m5w[var][:, i, :] = m[var][idx : idx + nperseg, :]\n",
    "\n",
    "print(\"Interpolating properties.\")\n",
    "# Do the interpolation\n",
    "for i in range(Nclevels):\n",
    "    # THIS hoffset is important!!!\n",
    "    c4[\"z\"][:, i] = np.mean(c12w[\"z\"][..., levis[i]], axis=(0, -1)) + hoffset[i]\n",
    "\n",
    "    for j in range(N_windows):\n",
    "        zr = c12w[\"z\"][:, j, levis[i]]\n",
    "        ur = c12w[\"u\"][:, j, levis[i]]\n",
    "        vr = c12w[\"v\"][:, j, levis[i]]\n",
    "        zi = c4[\"z\"][j, i]\n",
    "        c4w[\"z\"][:, j, i] = np.mean(zr, axis=-1)\n",
    "        c4w[\"t\"][:, j, i] = c12w[\"t\"][:, j, 0]\n",
    "        c4w[\"u\"][:, j, i] = moo.interp_quantity(zr, ur, zi, pzu)\n",
    "        c4w[\"v\"][:, j, i] = moo.interp_quantity(zr, vr, zi, pzv)\n",
    "\n",
    "        dudzr = np.gradient(ur, axis=-1) / np.gradient(zr, axis=-1)\n",
    "        dvdzr = np.gradient(vr, axis=-1) / np.gradient(zr, axis=-1)\n",
    "\n",
    "        # Instead of mean, could moo.interp1d\n",
    "        c4w[\"dudz\"][:, j, i] = np.mean(dudzr, axis=-1)\n",
    "        c4w[\"dvdz\"][:, j, i] = np.mean(dvdzr, axis=-1)\n",
    "\n",
    "        for m5w, m4w in zip(moorings5w, moorings4w):\n",
    "            zr = m5w[\"z\"][:, j, :]\n",
    "            ur = m5w[\"u\"][:, j, :]\n",
    "            vr = m5w[\"v\"][:, j, :]\n",
    "\n",
    "            m4w[\"z\"][:, j, i] = np.full((nperseg), zi)\n",
    "            m4w[\"u\"][:, j, i] = moo.interp_quantity(zr, ur, zi, pzu)\n",
    "            m4w[\"v\"][:, j, i] = moo.interp_quantity(zr, vr, zi, pzv)\n",
    "\n",
    "\n",
    "print(\"Filtering windowed data.\")\n",
    "fcorcpd = np.abs(gsw.f(cc[\"lat\"])) * 86400 / pi2\n",
    "\n",
    "varl = [\"u\", \"v\"]\n",
    "for var in varl:\n",
    "    c4w[var + \"_lo\"] = utils.butter_filter(\n",
    "        c4w[var], 24 / tc_hrs, fs=N_per_day, btype=\"low\", axis=0\n",
    "    )\n",
    "    c4w[var + \"_hi\"] = c4w[var] - c4w[var + \"_lo\"]\n",
    "\n",
    "varl = [\"u\", \"v\"]\n",
    "for var in varl:\n",
    "    for m4w in moorings4w:\n",
    "        m4w[var + \"_lo\"] = utils.butter_filter(\n",
    "            m4w[var], 24 / tc_hrs, fs=N_per_day, btype=\"low\", axis=0\n",
    "        )\n",
    "        m4w[var + \"_hi\"] = m4w[var] - m4w[var + \"_lo\"]\n",
    "\n",
    "c4w[\"zi\"] = np.ones_like(c4w[\"z\"]) * c4[\"z\"]\n",
    "\n",
    "print(\"Calculating horizontal gradients.\")\n",
    "# Calculate horizontal gradients\n",
    "for j in range(N_windows):\n",
    "    ll = np.stack(\n",
    "        ([m[\"lon\"] for m in moorings[1:]], [m[\"lat\"] for m in moorings[1:]]), axis=1\n",
    "    )\n",
    "    uv = np.stack(\n",
    "        (\n",
    "            [m4w[\"u_lo\"][:, j, :] for m4w in moorings4w],\n",
    "            [m4w[\"v_lo\"][:, j, :] for m4w in moorings4w],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    dudx, dudy, dvdx, dvdy, vort, div = moo.div_vort_4D(ll[:, 0], ll[:, 1], uv)\n",
    "    nstrain = dudx - dvdy\n",
    "    sstrain = dvdx + dudy\n",
    "    c4w[\"dudx\"][:, j, :] = dudx\n",
    "    c4w[\"dudy\"][:, j, :] = dudy\n",
    "    c4w[\"dvdx\"][:, j, :] = dvdx\n",
    "    c4w[\"dvdy\"][:, j, :] = dvdy\n",
    "    c4w[\"nstrain\"][:, j, :] = nstrain\n",
    "    c4w[\"sstrain\"][:, j, :] = sstrain\n",
    "    c4w[\"vort\"][:, j, :] = vort\n",
    "    c4w[\"div\"][:, j, :] = div\n",
    "\n",
    "for var in var4:\n",
    "    if var == \"z\":  # Keep z as modified by hoffset.\n",
    "        continue\n",
    "    c4[var] = np.mean(c4w[var], axis=0)\n",
    "\n",
    "freq, c4w[\"Puu\"] = sig.welch(c4w[\"u_hi\"], **spec_kwargs)\n",
    "_, c4w[\"Pvv\"] = sig.welch(c4w[\"v_hi\"], **spec_kwargs)\n",
    "_, c4w[\"Cuv\"] = sig.csd(c4w[\"u_hi\"], c4w[\"v_hi\"], **spec_kwargs)\n",
    "\n",
    "c4w[\"freq\"] = freq.copy()\n",
    "\n",
    "# Get rid of annoying tiny values.\n",
    "svarl = [\"Puu\", \"Pvv\", \"Cuv\"]\n",
    "for var in svarl:\n",
    "    c4w[var][0, ...] = 0.0\n",
    "    c4[var + \"_int\"] = np.full((N_windows, 4), np.nan)\n",
    "\n",
    "# Horizontal azimuth according to Jing 2018\n",
    "c4w[\"theta\"] = np.arctan2(2.0 * c4w[\"Cuv\"].real, (c4w[\"Puu\"] - c4w[\"Pvv\"])) / 2\n",
    "\n",
    "# Integration #############################################################\n",
    "print(\"Integrating power spectra.\")\n",
    "for var in svarl:\n",
    "    c4w[var + \"_cint\"] = np.full_like(c4w[var], fill_value=np.nan)\n",
    "\n",
    "fcor = np.abs(gsw.f(cc[\"lat\"])) / pi2\n",
    "N_freq = len(freq)\n",
    "freq_ = np.tile(freq[:, np.newaxis, np.newaxis], (1, N_windows, Nclevels))\n",
    "# ulim = fhi * np.tile(c4[\"N\"][np.newaxis, ...], (N_freq, 1, 1)) / pi2\n",
    "ulim = 1e9  # Set a huge upper limit since we don't know what N is...\n",
    "llim = fcor * flo\n",
    "use = (freq_ < ulim) & (freq_ > llim)\n",
    "\n",
    "svarl = [\"Puu\", \"Pvv\", \"Cuv\"]\n",
    "for var in svarl:\n",
    "    c4[var + \"_int\"] = igr.simps(use * c4w[var].real, freq, axis=0)\n",
    "    c4w[var + \"_cint\"] = igr.cumtrapz(use * c4w[var].real, freq, axis=0, initial=0.0)\n",
    "\n",
    "# Change lower integration limits for vertical components...\n",
    "llim = fcor * flov\n",
    "use = (freq_ < ulim) & (freq_ > llim)\n",
    "\n",
    "\n",
    "# Usefull quantities\n",
    "c4[\"nstress\"] = c4[\"Puu_int\"] - c4[\"Pvv_int\"]\n",
    "c4[\"sstress\"] = -2.0 * c4[\"Cuv_int\"]\n",
    "\n",
    "c4[\"F_horiz\"] = (\n",
    "    -0.5 * (c4[\"Puu_int\"] - c4[\"Pvv_int\"]) * c4[\"nstrain\"]\n",
    "    - c4[\"Cuv_int\"] * c4[\"sstrain\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ## Now we have to create the model 'truth'...\n",
    "#\n",
    "# Load the model data and estimate some gradients.\n",
    "\n",
    "print(\"Estimating smoothed gradients (slow).\")\n",
    "mluv = xr.load_dataset(\"../data/mooring_locations_uv1.nc\")\n",
    "mluv = mluv.isel(\n",
    "    t=slice(0, np.argwhere(mluv.u[:, 0, 0].data == 0)[0][0])\n",
    ")  # Get rid of end zeros...\n",
    "mluv = mluv.assign_coords(lon=mluv.lon)\n",
    "mluv = mluv.assign_coords(id=[\"cc\", \"nw\", \"ne\", \"se\", \"sw\"])\n",
    "mluv[\"dudz\"] = ([\"t\", \"z\", \"index\"], np.gradient(mluv.u, mluv.z, axis=1))\n",
    "mluv[\"dvdz\"] = ([\"t\", \"z\", \"index\"], np.gradient(mluv.v, mluv.z, axis=1))\n",
    "\n",
    "uv = np.rollaxis(np.stack((mluv.u, mluv.v))[..., 1:], 3, 0)\n",
    "dudx, dudy, dvdx, dvdy, vort, div = moo.div_vort_4D(mluv.lon[1:], mluv.lat[1:], uv)\n",
    "nstrain = dudx - dvdy\n",
    "sstrain = dvdx + dudy\n",
    "mluv[\"dudx\"] = ([\"t\", \"z\"], dudx)\n",
    "mluv[\"dudy\"] = ([\"t\", \"z\"], dudy)\n",
    "mluv[\"dvdx\"] = ([\"t\", \"z\"], dvdx)\n",
    "mluv[\"dvdy\"] = ([\"t\", \"z\"], dvdy)\n",
    "mluv[\"nstrain\"] = ([\"t\", \"z\"], nstrain)\n",
    "mluv[\"sstrain\"] = ([\"t\", \"z\"], sstrain)\n",
    "mluv[\"vort\"] = ([\"t\", \"z\"], vort)\n",
    "mluv[\"div\"] = ([\"t\", \"z\"], div)\n",
    "\n",
    "# Smooth the model data in an equivalent way to the real mooring.\n",
    "dudxs = (\n",
    "    mluv.dudx.rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "dvdxs = (\n",
    "    mluv.dvdx.rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "dudys = (\n",
    "    mluv.dudy.rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "dvdys = (\n",
    "    mluv.dvdy.rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "sstrains = (\n",
    "    mluv.sstrain.rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "nstrains = (\n",
    "    mluv.nstrain.rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "divs = (\n",
    "    mluv.div.rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "vorts = (\n",
    "    mluv.vort.rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "dudzs = (\n",
    "    mluv.dudz.isel(index=0)\n",
    "    .rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "dvdzs = (\n",
    "    mluv.dvdz.isel(index=0)\n",
    "    .rolling(t=nperseg, center=True)\n",
    "    .reduce(np.average, weights=sig.hann(nperseg))\n",
    "    .dropna(\"t\")\n",
    ")\n",
    "\n",
    "# Make spline fits.\n",
    "fdudx = itpl.RectBivariateSpline(dudxs.t.data, -dudxs.z.data, dudxs.data)\n",
    "fdvdx = itpl.RectBivariateSpline(dvdxs.t.data, -dvdxs.z.data, dvdxs.data)\n",
    "fdudy = itpl.RectBivariateSpline(dudys.t.data, -dudys.z.data, dudys.data)\n",
    "fdvdy = itpl.RectBivariateSpline(dvdys.t.data, -dvdys.z.data, dvdys.data)\n",
    "fsstrain = itpl.RectBivariateSpline(sstrains.t.data, -sstrains.z.data, sstrains.data)\n",
    "fnstrain = itpl.RectBivariateSpline(nstrains.t.data, -nstrains.z.data, nstrains.data)\n",
    "fdiv = itpl.RectBivariateSpline(divs.t.data, -divs.z.data, divs.data)\n",
    "fvort = itpl.RectBivariateSpline(vorts.t.data, -vorts.z.data, vorts.data)\n",
    "fdudz = itpl.RectBivariateSpline(dudzs.t.data, -dudzs.z.data, dudzs.data)\n",
    "fdvdz = itpl.RectBivariateSpline(dvdzs.t.data, -dvdzs.z.data, dvdzs.data)\n",
    "\n",
    "# Interpolate using splines.\n",
    "dudxt = fdudx(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "dvdxt = fdvdx(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "dudyt = fdudy(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "dvdyt = fdvdy(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "sstraint = fsstrain(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "nstraint = fnstrain(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "divt = fdiv(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "vortt = fvort(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "dudzt = fdudz(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "dvdzt = fdvdz(c4[\"t\"], -c4[\"z\"], grid=False)\n",
    "\n",
    "c4[\"dudxt\"] = dudxt\n",
    "c4[\"dvdxt\"] = dvdxt\n",
    "c4[\"dudyt\"] = dudyt\n",
    "c4[\"dvdyt\"] = dvdyt\n",
    "c4[\"sstraint\"] = sstraint\n",
    "c4[\"nstraint\"] = nstraint\n",
    "c4[\"divt\"] = divt\n",
    "c4[\"vortt\"] = vortt\n",
    "c4[\"dudzt\"] = dudzt\n",
    "c4[\"dvdzt\"] = dvdzt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "########################## SAVE CORRECTED FILES ##########################"
   },
   "outputs": [],
   "source": [
    "io.savemat(\"../data/virtual_mooring_interpolated.mat\", c4)\n",
    "io.savemat(\"../data/virtual_mooring_interpolated_windowed.mat\", c4w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signal to noise ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimating signal to noise ratios.\")\n",
    "\n",
    "M = munch.munchify(utils.loadmat('../data/virtual_mooring_interpolated.mat'))\n",
    "\n",
    "# shear strain\n",
    "dsstrain = M.sstrain - M.sstraint\n",
    "SNR_sstrain = M.sstrain.var(axis=0)/dsstrain.var(axis=0)\n",
    "np.save('../data/SNR_sstrain', SNR_sstrain, allow_pickle=False)\n",
    "\n",
    "# normal strain\n",
    "dnstrain = M.nstrain - M.nstraint\n",
    "SNR_nstrain = M.nstrain.var(axis=0)/dnstrain.var(axis=0)\n",
    "np.save('../data/SNR_nstrain', SNR_nstrain, allow_pickle=False)\n",
    "\n",
    "# zonal shear\n",
    "ddudz = M.dudz - M.dudzt\n",
    "SNR_dudz = M.dvdz.var(axis=0)/ddudz.var(axis=0)\n",
    "np.save('../data/SNR_dudz', SNR_dudz, allow_pickle=False)\n",
    "\n",
    "# meridional shear\n",
    "ddvdz = M.dvdz - M.dvdzt\n",
    "SNR_dvdz = M.dvdz.var(axis=0)/ddvdz.var(axis=0)\n",
    "np.save('../data/SNR_dvdz', SNR_dvdz, allow_pickle=False)\n",
    "\n",
    "# divergence\n",
    "ddiv = M.div - M.divt\n",
    "SNR_nstrain = M.div.var(axis=0)/ddiv.var(axis=0)\n",
    "np.save('../data/SNR_div', SNR_nstrain, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corrected\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate interpolated data.\n",
    "\n",
    "Set parameters again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected levels.\n",
    "# heights = [-540., -1250., -2100., -3500.]\n",
    "# Filter cut off (hours)\n",
    "tc_hrs = 40.0\n",
    "# Start of time series (matlab datetime)\n",
    "t_start = 734494.0\n",
    "# Length of time series\n",
    "max_len = N_data = 42048\n",
    "# Data file\n",
    "raw_data_file = \"moorings.mat\"\n",
    "# Index where NaNs start in u and v data from SW mooring\n",
    "sw_vel_nans = 14027\n",
    "# Sampling period (minutes)\n",
    "dt_min = 15.0\n",
    "# Window length for wave stress quantities and mesoscale strain quantities.\n",
    "nperseg = 2 ** 9\n",
    "# Spectra parameters\n",
    "window = \"hanning\"\n",
    "detrend = \"constant\"\n",
    "# Extrapolation/interpolation limit above which data will be removed.\n",
    "dzlim = 100.0\n",
    "# Integration of spectra parameters. These multiple N and f respectively to set\n",
    "# the integration limits.\n",
    "fhi = 1.0\n",
    "flo = 1.0\n",
    "flov = 1.0  # When integrating spectra involved in vertical fluxes, get rid of\n",
    "# the near inertial portion.\n",
    "# When bandpass filtering windowed data use these params multiplied by f and N\n",
    "filtlo = 0.9  # times f\n",
    "filthi = 1.1  # times N\n",
    "\n",
    "# Interpolation distance that raises flag (m)\n",
    "zimax = 100.0\n",
    "\n",
    "dt_sec = dt_min * 60.0  # Sample period in seconds.\n",
    "dt_day = dt_sec / 86400.0  # Sample period in days.\n",
    "N_per_day = int(1.0 / dt_day)  # Samples per day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial fits first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"REAL MOORING INTERPOLATION\")\n",
    "print(\"**Generating corrected data**\")\n",
    "\n",
    "moorings = load_data.load_my_data()\n",
    "cc, nw, ne, se, sw = moorings\n",
    "\n",
    "# Generate corrected moorings\n",
    "T = np.concatenate([m[\"T\"].flatten() for m in moorings])\n",
    "S = np.concatenate([m[\"S\"].flatten() for m in moorings])\n",
    "z = np.concatenate([m[\"z\"].flatten() for m in moorings])\n",
    "u = np.concatenate([m[\"u\"].flatten() for m in moorings])\n",
    "v = np.concatenate([m[\"v\"].flatten() for m in moorings])\n",
    "g = np.concatenate([m[\"gamman\"].flatten() for m in moorings])\n",
    "\n",
    "# SW problems...\n",
    "nans = np.isnan(u) | np.isnan(v)\n",
    "\n",
    "print(\"Calculating polynomial coefficients.\")\n",
    "pzT = np.polyfit(z[~nans], T[~nans], 3)\n",
    "pzS = np.polyfit(z[~nans], S[~nans], 3)\n",
    "pzg = np.polyfit(z[~nans], g[~nans], 3)\n",
    "pzu = np.polyfit(z[~nans], u[~nans], 2)\n",
    "pzv = np.polyfit(z[~nans], v[~nans], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional height in m to add to interpolation height.\n",
    "hoffset = [-25.0, 50.0, -50.0, 100.0]\n",
    "\n",
    "pi2 = np.pi * 2.0\n",
    "nfft = nperseg\n",
    "levis = [(0, 1, 2, 3), (4, 5), (6, 7, 8, 9), (10, 11)]\n",
    "Nclevels = len(levis)\n",
    "spec_kwargs = {\n",
    "    \"fs\": 1.0 / dt_sec,\n",
    "    \"window\": window,\n",
    "    \"nperseg\": nperseg,\n",
    "    \"nfft\": nfft,\n",
    "    \"detrend\": detrend,\n",
    "    \"axis\": 0,\n",
    "}\n",
    "\n",
    "idx1 = np.arange(nperseg, N_data, nperseg // 2)  # Window end index\n",
    "idx0 = idx1 - nperseg  # Window start index\n",
    "N_windows = len(idx0)\n",
    "\n",
    "# Initialise the place holder dictionaries.\n",
    "c12w = {\"N_levels\": 12}  # Dictionary for raw, windowed data from central mooring\n",
    "c4w = {\"N_levels\": Nclevels}  # Dictionary for processed, windowed data\n",
    "c4 = {\"N_levels\": Nclevels}  # Dictionary for processed data\n",
    "# Dictionaries for raw, windowed data from outer moorings\n",
    "nw5w, ne5w, se5w, sw5w = {\"id\": \"nw\"}, {\"id\": \"ne\"}, {\"id\": \"se\"}, {\"id\": \"sw\"}\n",
    "moorings5w = [nw5w, ne5w, se5w, sw5w]\n",
    "# Dictionaries for processed, windowed data from outer moorings\n",
    "nw4w, ne4w, se4w, sw4w = {\"id\": \"nw\"}, {\"id\": \"ne\"}, {\"id\": \"se\"}, {\"id\": \"sw\"}\n",
    "moorings4w = [nw4w, ne4w, se4w, sw4w]\n",
    "\n",
    "# Initialised the arrays of windowed data\n",
    "varr = [\"t\", \"z\", \"u\", \"v\", \"gamman\", \"S\", \"T\", \"P\"]\n",
    "for var in varr:\n",
    "    c12w[var] = np.zeros((nperseg, N_windows, cc[\"N_levels\"]))\n",
    "\n",
    "var4 = [\n",
    "    \"t\",\n",
    "    \"z\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"gamman\",\n",
    "    \"dudx\",\n",
    "    \"dvdx\",\n",
    "    \"dudy\",\n",
    "    \"dvdy\",\n",
    "    \"dudz\",\n",
    "    \"dvdz\",\n",
    "    \"dgdz\",\n",
    "    \"nstrain\",\n",
    "    \"sstrain\",\n",
    "    \"vort\",\n",
    "    \"N2\",\n",
    "]\n",
    "for var in var4:\n",
    "    c4w[var] = np.zeros((nperseg, N_windows, Nclevels))\n",
    "\n",
    "for var in var4:\n",
    "    c4[var] = np.zeros((N_windows, Nclevels))\n",
    "\n",
    "# Initialised the arrays of windowed data for outer mooring\n",
    "varro = [\"z\", \"u\", \"v\"]\n",
    "for var in varro:\n",
    "    for m5w in moorings5w:\n",
    "        m5w[var] = np.zeros((nperseg, N_windows, 5))\n",
    "\n",
    "var4o = [\"z\", \"u\", \"v\"]\n",
    "for var in var4o:\n",
    "    for m4w in moorings4w:\n",
    "        m4w[var] = np.zeros((nperseg, N_windows, Nclevels))\n",
    "\n",
    "# for var in var4o:\n",
    "#     for m4 in moorings4:\n",
    "#         m4[var] = np.zeros((N_windows, 4))\n",
    "\n",
    "# Window the raw data.\n",
    "for i in range(N_windows):\n",
    "    idx = idx0[i]\n",
    "    for var in varr:\n",
    "        c12w[var][:, i, :] = cc[var][idx : idx + nperseg, :]\n",
    "\n",
    "for i in range(N_windows):\n",
    "    idx = idx0[i]\n",
    "    for var in varro:\n",
    "        for m5w, m in zip(moorings5w, moorings[1:]):\n",
    "            m5w[var][:, i, :] = m[var][idx : idx + nperseg, :]\n",
    "\n",
    "c4[\"interp_far_flag\"] = np.full_like(c4[\"u\"], False, dtype=bool)\n",
    "\n",
    "print(\"Interpolating properties.\")\n",
    "# Do the interpolation\n",
    "for i in range(Nclevels):\n",
    "    # THIS hoffset is important!!!\n",
    "    c4[\"z\"][:, i] = np.mean(c12w[\"z\"][..., levis[i]], axis=(0, -1)) + hoffset[i]\n",
    "\n",
    "    for j in range(N_windows):\n",
    "        zr = c12w[\"z\"][:, j, levis[i]]\n",
    "        ur = c12w[\"u\"][:, j, levis[i]]\n",
    "        vr = c12w[\"v\"][:, j, levis[i]]\n",
    "        gr = c12w[\"gamman\"][:, j, levis[i]]\n",
    "        Sr = c12w[\"S\"][:, j, levis[i]]\n",
    "        Tr = c12w[\"T\"][:, j, levis[i]]\n",
    "        Pr = c12w[\"P\"][:, j, levis[i]]\n",
    "        zi = c4[\"z\"][j, i]\n",
    "\n",
    "        c4[\"interp_far_flag\"][j, i] = np.any(np.min(np.abs(zr - zi), axis=-1) > zimax)\n",
    "\n",
    "        c4w[\"z\"][:, j, i] = np.mean(zr, axis=-1)\n",
    "        c4w[\"t\"][:, j, i] = c12w[\"t\"][:, j, 0]\n",
    "        c4w[\"u\"][:, j, i] = moo.interp_quantity(zr, ur, zi, pzu)\n",
    "        c4w[\"v\"][:, j, i] = moo.interp_quantity(zr, vr, zi, pzv)\n",
    "        c4w[\"gamman\"][:, j, i] = moo.interp_quantity(zr, gr, zi, pzg)\n",
    "\n",
    "        dudzr = np.gradient(ur, axis=-1) / np.gradient(zr, axis=-1)\n",
    "        dvdzr = np.gradient(vr, axis=-1) / np.gradient(zr, axis=-1)\n",
    "        dgdzr = np.gradient(gr, axis=-1) / np.gradient(zr, axis=-1)\n",
    "        N2 = seawater.bfrq(Sr.T, Tr.T, Pr.T, cc[\"lat\"])[0].T\n",
    "\n",
    "        # Instead of mean, could moo.interp1d\n",
    "        c4w[\"dudz\"][:, j, i] = np.mean(dudzr, axis=-1)\n",
    "        c4w[\"dvdz\"][:, j, i] = np.mean(dvdzr, axis=-1)\n",
    "        c4w[\"dgdz\"][:, j, i] = np.mean(dgdzr, axis=-1)\n",
    "        c4w[\"N2\"][:, j, i] = np.mean(N2, axis=-1)\n",
    "\n",
    "        for m5w, m4w in zip(moorings5w, moorings4w):\n",
    "            if (m5w[\"id\"] == \"sw\") & (\n",
    "                idx1[j] > sw_vel_nans\n",
    "            ):  # Skip this level because of NaNs\n",
    "                zr = m5w[\"z\"][:, j, (0, 1, 3, 4)]\n",
    "                ur = m5w[\"u\"][:, j, (0, 1, 3, 4)]\n",
    "                vr = m5w[\"v\"][:, j, (0, 1, 3, 4)]\n",
    "            else:\n",
    "                zr = m5w[\"z\"][:, j, :]\n",
    "                ur = m5w[\"u\"][:, j, :]\n",
    "                vr = m5w[\"v\"][:, j, :]\n",
    "\n",
    "            m4w[\"z\"][:, j, i] = np.full((nperseg), zi)\n",
    "            m4w[\"u\"][:, j, i] = moo.interp_quantity(zr, ur, zi, pzu)\n",
    "            m4w[\"v\"][:, j, i] = moo.interp_quantity(zr, vr, zi, pzv)\n",
    "\n",
    "print(\"Filtering windowed data.\")\n",
    "fcorcpd = np.abs(cc[\"f\"]) * 86400 / pi2\n",
    "\n",
    "Nmean = np.sqrt(np.average(c4w[\"N2\"], weights=sig.hann(nperseg), axis=0))\n",
    "\n",
    "varl = [\"u\", \"v\", \"gamman\"]\n",
    "for var in varl:\n",
    "    c4w[var + \"_hib\"] = np.zeros_like(c4w[var])\n",
    "    c4w[var + \"_lo\"] = utils.butter_filter(\n",
    "        c4w[var], 24 / tc_hrs, fs=N_per_day, btype=\"low\", axis=0\n",
    "    )\n",
    "    c4w[var + \"_hi\"] = c4w[var] - c4w[var + \"_lo\"]\n",
    "\n",
    "for i in range(Nclevels):\n",
    "    for j in range(N_windows):\n",
    "        Nmean_ = Nmean[j, i] * 86400 / pi2\n",
    "        for var in varl:\n",
    "            c4w[var + \"_hib\"][:, j, i] = utils.butter_filter(\n",
    "                c4w[var][:, j, i],\n",
    "                (filtlo * fcorcpd, filthi * Nmean_),\n",
    "                fs=N_per_day,\n",
    "                btype=\"band\",\n",
    "            )\n",
    "\n",
    "varl = [\"u\", \"v\"]\n",
    "for var in varl:\n",
    "    for m4w in moorings4w:\n",
    "        m4w[var + \"_lo\"] = utils.butter_filter(\n",
    "            m4w[var], 24 / tc_hrs, fs=N_per_day, btype=\"low\", axis=0\n",
    "        )\n",
    "        m4w[var + \"_hi\"] = m4w[var] - m4w[var + \"_lo\"]\n",
    "\n",
    "c4w[\"zi\"] = np.ones_like(c4w[\"z\"]) * c4[\"z\"]\n",
    "\n",
    "\n",
    "print(\"Calculating horizontal gradients.\")\n",
    "# Calculate horizontal gradients\n",
    "for j in range(N_windows):\n",
    "    ll = np.stack(\n",
    "        ([m[\"lon\"] for m in moorings[1:]], [m[\"lat\"] for m in moorings[1:]]), axis=1\n",
    "    )\n",
    "    uv = np.stack(\n",
    "        (\n",
    "            [m4w[\"u_lo\"][:, j, :] for m4w in moorings4w],\n",
    "            [m4w[\"v_lo\"][:, j, :] for m4w in moorings4w],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    dudx, dudy, dvdx, dvdy, vort, _ = moo.div_vort_4D(ll[:, 0], ll[:, 1], uv)\n",
    "    nstrain = dudx - dvdy\n",
    "    sstrain = dvdx + dudy\n",
    "    c4w[\"dudx\"][:, j, :] = dudx\n",
    "    c4w[\"dudy\"][:, j, :] = dudy\n",
    "    c4w[\"dvdx\"][:, j, :] = dvdx\n",
    "    c4w[\"dvdy\"][:, j, :] = dvdy\n",
    "    c4w[\"nstrain\"][:, j, :] = nstrain\n",
    "    c4w[\"sstrain\"][:, j, :] = sstrain\n",
    "    c4w[\"vort\"][:, j, :] = vort\n",
    "\n",
    "print(\"Calculating window averages.\")\n",
    "for var in var4 + [\"u_lo\", \"v_lo\", \"gamman_lo\"]:\n",
    "    if var == \"z\":  # Keep z as modified by hoffset.\n",
    "        continue\n",
    "    c4[var] = np.average(c4w[var], weights=sig.hann(nperseg), axis=0)\n",
    "\n",
    "print(\"Estimating w and b.\")\n",
    "om = np.fft.fftfreq(nperseg, 15 * 60)\n",
    "c4w[\"w_hi\"] = np.fft.ifft(\n",
    "    1j\n",
    "    * pi2\n",
    "    * om[:, np.newaxis, np.newaxis]\n",
    "    * np.fft.fft(-c4w[\"gamman_hi\"] / c4[\"dgdz\"], axis=0),\n",
    "    axis=0,\n",
    ").real\n",
    "c4w[\"w_hib\"] = np.fft.ifft(\n",
    "    1j\n",
    "    * pi2\n",
    "    * om[:, np.newaxis, np.newaxis]\n",
    "    * np.fft.fft(-c4w[\"gamman_hib\"] / c4[\"dgdz\"], axis=0),\n",
    "    axis=0,\n",
    ").real\n",
    "\n",
    "# Estimate buoyancy variables\n",
    "c4w[\"b_hi\"] = -gsw.grav(-c4[\"z\"], cc[\"lat\"]) * c4w[\"gamman_hi\"] / c4[\"gamman_lo\"]\n",
    "c4w[\"b_hib\"] = -gsw.grav(-c4[\"z\"], cc[\"lat\"]) * c4w[\"gamman_hib\"] / c4[\"gamman_lo\"]\n",
    "c4[\"N\"] = np.sqrt(c4[\"N2\"])\n",
    "\n",
    "print(\"Estimating covariance spectra.\")\n",
    "freq, c4w[\"Puu\"] = sig.welch(c4w[\"u_hi\"], **spec_kwargs)\n",
    "_, c4w[\"Pvv\"] = sig.welch(c4w[\"v_hi\"], **spec_kwargs)\n",
    "_, c4w[\"Pww\"] = sig.welch(c4w[\"w_hi\"], **spec_kwargs)\n",
    "_, c4w[\"Pwwg\"] = sig.welch(c4w[\"gamman_hi\"] / c4[\"dgdz\"], **spec_kwargs)\n",
    "c4w[\"Pwwg\"] *= (pi2 * freq[:, np.newaxis, np.newaxis]) ** 2\n",
    "_, c4w[\"Pbb\"] = sig.welch(c4w[\"b_hi\"], **spec_kwargs)\n",
    "_, c4w[\"Cuv\"] = sig.csd(c4w[\"u_hi\"], c4w[\"v_hi\"], **spec_kwargs)\n",
    "_, c4w[\"Cuwg\"] = sig.csd(c4w[\"u_hi\"], c4w[\"gamman_hi\"] / c4[\"dgdz\"], **spec_kwargs)\n",
    "c4w[\"Cuwg\"] *= -1j * pi2 * freq[:, np.newaxis, np.newaxis]\n",
    "_, c4w[\"Cvwg\"] = sig.csd(c4w[\"v_hi\"], c4w[\"gamman_hi\"] / c4[\"dgdz\"], **spec_kwargs)\n",
    "c4w[\"Cvwg\"] *= -1j * pi2 * freq[:, np.newaxis, np.newaxis]\n",
    "_, c4w[\"Cub\"] = sig.csd(c4w[\"u_hi\"], c4w[\"b_hi\"], **spec_kwargs)\n",
    "_, c4w[\"Cvb\"] = sig.csd(c4w[\"v_hi\"], c4w[\"b_hi\"], **spec_kwargs)\n",
    "\n",
    "print(\"Estimating covariance matrices.\")\n",
    "\n",
    "\n",
    "def cov(x, y, axis=None):\n",
    "    return np.mean((x - np.mean(x, axis=axis)) * (y - np.mean(y, axis=axis)), axis=axis)\n",
    "\n",
    "\n",
    "c4[\"couu\"] = cov(c4w[\"u_hib\"], c4w[\"u_hib\"], axis=0)\n",
    "c4[\"covv\"] = cov(c4w[\"v_hib\"], c4w[\"v_hib\"], axis=0)\n",
    "c4[\"coww\"] = cov(c4w[\"w_hib\"], c4w[\"w_hib\"], axis=0)\n",
    "c4[\"cobb\"] = cov(c4w[\"b_hib\"], c4w[\"b_hib\"], axis=0)\n",
    "c4[\"couv\"] = cov(c4w[\"u_hib\"], c4w[\"v_hib\"], axis=0)\n",
    "c4[\"couw\"] = cov(c4w[\"u_hib\"], c4w[\"w_hib\"], axis=0)\n",
    "c4[\"covw\"] = cov(c4w[\"v_hib\"], c4w[\"w_hib\"], axis=0)\n",
    "c4[\"coub\"] = cov(c4w[\"u_hib\"], c4w[\"b_hib\"], axis=0)\n",
    "c4[\"covb\"] = cov(c4w[\"v_hib\"], c4w[\"b_hib\"], axis=0)\n",
    "\n",
    "c4w[\"freq\"] = freq.copy()\n",
    "\n",
    "# Get rid of annoying tiny values.\n",
    "svarl = [\"Puu\", \"Pvv\", \"Pbb\", \"Cuv\", \"Cub\", \"Cvb\", \"Pwwg\", \"Cuwg\", \"Cvwg\"]\n",
    "for var in svarl:\n",
    "    c4w[var][0, ...] = 0.0\n",
    "    c4[var + \"_int\"] = np.full((N_windows, 4), np.nan)\n",
    "\n",
    "# Horizontal azimuth according to Jing 2018\n",
    "c4w[\"theta\"] = np.arctan2(2.0 * c4w[\"Cuv\"].real, (c4w[\"Puu\"] - c4w[\"Pvv\"])) / 2\n",
    "\n",
    "# Integration #############################################################\n",
    "print(\"Integrating power spectra.\")\n",
    "for var in svarl:\n",
    "    c4w[var + \"_cint\"] = np.full_like(c4w[var], fill_value=np.nan)\n",
    "\n",
    "fcor = np.abs(cc[\"f\"]) / pi2\n",
    "N_freq = len(freq)\n",
    "freq_ = np.tile(freq[:, np.newaxis, np.newaxis], (1, N_windows, Nclevels))\n",
    "ulim = fhi * np.tile(c4[\"N\"][np.newaxis, ...], (N_freq, 1, 1)) / pi2\n",
    "llim = fcor * flo\n",
    "use = (freq_ < ulim) & (freq_ > llim)\n",
    "\n",
    "svarl = [\"Puu\", \"Pvv\", \"Pbb\", \"Cuv\", \"Pwwg\"]\n",
    "for var in svarl:\n",
    "    c4[var + \"_int\"] = igr.simps(use * c4w[var].real, freq, axis=0)\n",
    "    c4w[var + \"_cint\"] = igr.cumtrapz(use * c4w[var].real, freq, axis=0, initial=0.0)\n",
    "\n",
    "# Change lower integration limits for vertical components...\n",
    "llim = fcor * flov\n",
    "use = (freq_ < ulim) & (freq_ > llim)\n",
    "\n",
    "svarl = [\"Cub\", \"Cvb\", \"Cuwg\", \"Cvwg\"]\n",
    "for var in svarl:\n",
    "    c4[var + \"_int\"] = igr.simps(use * c4w[var].real, freq, axis=0)\n",
    "    c4w[var + \"_cint\"] = igr.cumtrapz(use * c4w[var].real, freq, axis=0, initial=0.0)\n",
    "\n",
    "# Ruddic and Joyce effective stress\n",
    "for var1, var2 in zip([\"Tuwg\", \"Tvwg\"], [\"Cuwg\", \"Cvwg\"]):\n",
    "    func = use * c4w[var2].real * (1 - fcor ** 2 / freq_ ** 2)\n",
    "    nans = np.isnan(func)\n",
    "    func[nans] = 0.0\n",
    "    c4[var1 + \"_int\"] = igr.simps(func, freq, axis=0)\n",
    "    func = use * c4w[var2].real * (1 - fcor ** 2 / freq_ ** 2)\n",
    "    nans = np.isnan(func)\n",
    "    func[nans] = 0.0\n",
    "    c4w[var1 + \"_cint\"] = igr.cumtrapz(func, freq, axis=0, initial=0.0)\n",
    "\n",
    "# Usefull quantities\n",
    "c4[\"nstress\"] = c4[\"Puu_int\"] - c4[\"Pvv_int\"]\n",
    "c4[\"sstress\"] = -2.0 * c4[\"Cuv_int\"]\n",
    "\n",
    "c4[\"F_horiz\"] = (\n",
    "    -0.5 * (c4[\"Puu_int\"] - c4[\"Pvv_int\"]) * c4[\"nstrain\"]\n",
    "    - c4[\"Cuv_int\"] * c4[\"sstrain\"]\n",
    ")\n",
    "c4[\"F_vert\"] = (\n",
    "    -(c4[\"Cuwg_int\"] - cc[\"f\"] * c4[\"Cvb_int\"] / c4[\"N\"] ** 2) * c4[\"dudz\"]\n",
    "    - (c4[\"Cvwg_int\"] + cc[\"f\"] * c4[\"Cub_int\"] / c4[\"N\"] ** 2) * c4[\"dvdz\"]\n",
    ")\n",
    "c4[\"F_vert_alt\"] = -c4[\"Tuwg_int\"] * c4[\"dudz\"] - c4[\"Tvwg_int\"] * c4[\"dvdz\"]\n",
    "\n",
    "c4[\"F_total\"] = c4[\"F_horiz\"] + c4[\"F_vert\"]\n",
    "\n",
    "c4[\"EPu\"] = c4[\"Cuwg_int\"] - cc[\"f\"] * c4[\"Cvb_int\"] / c4[\"N\"] ** 2\n",
    "c4[\"EPv\"] = c4[\"Cvwg_int\"] + cc[\"f\"] * c4[\"Cub_int\"] / c4[\"N\"] ** 2\n",
    "\n",
    "##\n",
    "\n",
    "c4[\"nstress_cov\"] = c4[\"couu\"] - c4[\"covv\"]\n",
    "c4[\"sstress_cov\"] = -2.0 * c4[\"couv\"]\n",
    "\n",
    "c4[\"F_horiz_cov\"] = (\n",
    "    -0.5 * (c4[\"couu\"] - c4[\"covv\"]) * c4[\"nstrain\"] - c4[\"couv\"] * c4[\"sstrain\"]\n",
    ")\n",
    "\n",
    "c4[\"F_vert_cov\"] = (\n",
    "    -(c4[\"couw\"] - cc[\"f\"] * c4[\"covb\"] / c4[\"N\"] ** 2) * c4[\"dudz\"]\n",
    "    - (c4[\"covw\"] + cc[\"f\"] * c4[\"coub\"] / c4[\"N\"] ** 2) * c4[\"dvdz\"]\n",
    ")\n",
    "\n",
    "c4[\"F_total_cov\"] = c4[\"F_horiz_cov\"] + c4[\"F_vert_cov\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate standard error on covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootnum = 1000\n",
    "np.random.seed(12341555)\n",
    "idxs = np.arange(nperseg, dtype=\"i2\")\n",
    "\n",
    "# def cov1(xy, axis=0):\n",
    "#     x = xy[..., -1]\n",
    "#     y = xy[..., -1]\n",
    "#     return np.mean((x - np.mean(x, axis=axis))*(y - np.mean(y, axis=axis)), axis=axis)\n",
    "\n",
    "print(\"Estimating error on covariance using bootstrap (slow).\")\n",
    "\n",
    "euu_ = np.zeros((bootnum, N_windows, Nclevels))\n",
    "evv_ = np.zeros((bootnum, N_windows, Nclevels))\n",
    "eww_ = np.zeros((bootnum, N_windows, Nclevels))\n",
    "ebb_ = np.zeros((bootnum, N_windows, Nclevels))\n",
    "euv_ = np.zeros((bootnum, N_windows, Nclevels))\n",
    "euw_ = np.zeros((bootnum, N_windows, Nclevels))\n",
    "evw_ = np.zeros((bootnum, N_windows, Nclevels))\n",
    "eub_ = np.zeros((bootnum, N_windows, Nclevels))\n",
    "evb_ = np.zeros((bootnum, N_windows, Nclevels))\n",
    "\n",
    "\n",
    "for i in range(bootnum):\n",
    "    idxs_ = np.random.choice(idxs, nperseg)\n",
    "\n",
    "    u_ = c4w[\"u_hib\"][idxs_, ...]\n",
    "    v_ = c4w[\"v_hib\"][idxs_, ...]\n",
    "    w_ = c4w[\"w_hib\"][idxs_, ...]\n",
    "    b_ = c4w[\"b_hib\"][idxs_, ...]\n",
    "\n",
    "    euu_[i, ...] = cov(u_, u_, axis=0)\n",
    "    evv_[i, ...] = cov(v_, v_, axis=0)\n",
    "    eww_[i, ...] = cov(w_, w_, axis=0)\n",
    "    ebb_[i, ...] = cov(b_, b_, axis=0)\n",
    "    euv_[i, ...] = cov(u_, v_, axis=0)\n",
    "    euw_[i, ...] = cov(u_, w_, axis=0)\n",
    "    evw_[i, ...] = cov(v_, w_, axis=0)\n",
    "    eub_[i, ...] = cov(u_, b_, axis=0)\n",
    "    evb_[i, ...] = cov(v_, b_, axis=0)\n",
    "\n",
    "c4[\"euu\"] = euu_.std(axis=0)\n",
    "c4[\"evv\"] = evv_.std(axis=0)\n",
    "c4[\"eww\"] = eww_.std(axis=0)\n",
    "c4[\"ebb\"] = ebb_.std(axis=0)\n",
    "c4[\"euv\"] = euv_.std(axis=0)\n",
    "c4[\"euw\"] = euw_.std(axis=0)\n",
    "c4[\"evw\"] = evw_.std(axis=0)\n",
    "c4[\"eub\"] = eub_.std(axis=0)\n",
    "c4[\"evb\"] = evb_.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error on gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finite_diff_err = 0.06  # Assume 6 percent...\n",
    "\n",
    "SNR_dudz = np.load(\"../data/SNR_dudz.npy\")\n",
    "SNR_dvdz = np.load(\"../data/SNR_dvdz.npy\")\n",
    "SNR_nstrain = np.load(\"../data/SNR_nstrain.npy\")\n",
    "SNR_sstrain = np.load(\"../data/SNR_sstrain.npy\")\n",
    "\n",
    "ones = np.ones_like(c4[\"euu\"])\n",
    "\n",
    "c4[\"edudz\"] = ones * np.sqrt(c4[\"dudz\"].var(axis=0) / SNR_dudz)\n",
    "c4[\"edvdz\"] = ones * np.sqrt(c4[\"dvdz\"].var(axis=0) / SNR_dvdz)\n",
    "c4[\"enstrain\"] = esum(\n",
    "    ones * np.sqrt(c4[\"nstrain\"].var(axis=0) / SNR_nstrain),\n",
    "    finite_diff_err * c4[\"nstrain\"],\n",
    ")\n",
    "c4[\"esstrain\"] = esum(\n",
    "    ones * np.sqrt(c4[\"sstrain\"].var(axis=0) / SNR_sstrain),\n",
    "    finite_diff_err * c4[\"sstrain\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euumvv = 0.5 * esum(c4[\"euu\"], c4[\"evv\"])\n",
    "c4[\"enstress\"] = euumvv.copy()\n",
    "enorm = emult(\n",
    "    -0.5 * (c4[\"Puu_int\"] - c4[\"Pvv_int\"]), c4[\"nstrain\"], euumvv, c4[\"enstrain\"]\n",
    ")\n",
    "eshear = emult(c4[\"Cuv_int\"], c4[\"sstrain\"], c4[\"euv\"], c4[\"esstrain\"])\n",
    "c4[\"errF_horiz_norm\"] = enorm.copy()\n",
    "c4[\"errF_horiz_shear\"] = eshear.copy()\n",
    "c4[\"errF_horiz\"] = esum(enorm, eshear)\n",
    "\n",
    "euumvv = 0.5 * esum(c4[\"euu\"], c4[\"evv\"])\n",
    "c4[\"enstress_cov\"] = euumvv.copy()\n",
    "enorm = emult(-0.5 * (c4[\"couu\"] - c4[\"covv\"]), c4[\"nstrain\"], euumvv, c4[\"enstrain\"])\n",
    "eshear = emult(c4[\"couv\"], c4[\"sstrain\"], c4[\"euv\"], c4[\"esstrain\"])\n",
    "c4[\"errF_horiz_norm_cov\"] = enorm.copy()\n",
    "c4[\"errF_horiz_shear_cov\"] = eshear.copy()\n",
    "c4[\"errF_horiz_cov\"] = esum(enorm, eshear)\n",
    "\n",
    "euwmvb = esum(c4[\"euw\"], np.abs(cc[\"f\"] / c4[\"N\"] ** 2) * c4[\"evb\"])\n",
    "evwpub = esum(c4[\"evw\"], np.abs(cc[\"f\"] / c4[\"N\"] ** 2) * c4[\"eub\"])\n",
    "c4[\"evstressu\"] = euwmvb\n",
    "c4[\"evstressv\"] = evwpub\n",
    "edu = emult(\n",
    "    -(c4[\"Cuwg_int\"] - cc[\"f\"] * c4[\"Cvb_int\"] / c4[\"N\"] ** 2),\n",
    "    c4[\"dudz\"],\n",
    "    euwmvb,\n",
    "    c4[\"edudz\"],\n",
    ")\n",
    "edv = emult(\n",
    "    -(c4[\"Cvwg_int\"] + cc[\"f\"] * c4[\"Cub_int\"] / c4[\"N\"] ** 2),\n",
    "    c4[\"dvdz\"],\n",
    "    evwpub,\n",
    "    c4[\"edvdz\"],\n",
    ")\n",
    "c4[\"errEPu\"] = edu.copy()\n",
    "c4[\"errEPv\"] = edv.copy()\n",
    "c4[\"errF_vert\"] = esum(edu, edv)\n",
    "\n",
    "c4[\"errEPu_alt\"] = emult(-c4[\"Tuwg_int\"], c4[\"dudz\"], c4[\"euw\"], c4[\"edudz\"])\n",
    "c4[\"errEPv_alt\"] = emult(-c4[\"Tvwg_int\"], c4[\"dvdz\"], c4[\"evw\"], c4[\"edvdz\"])\n",
    "c4[\"errF_vert_alt\"] = esum(c4[\"errEPu_alt\"], c4[\"errEPv_alt\"])\n",
    "\n",
    "edu = emult(\n",
    "    -(c4[\"couw\"] - cc[\"f\"] * c4[\"covb\"] / c4[\"N\"] ** 2), c4[\"dudz\"], euwmvb, c4[\"edudz\"]\n",
    ")\n",
    "edv = emult(\n",
    "    -(c4[\"covw\"] + cc[\"f\"] * c4[\"coub\"] / c4[\"N\"] ** 2), c4[\"dvdz\"], evwpub, c4[\"edvdz\"]\n",
    ")\n",
    "c4[\"errEPu_cov\"] = edu.copy()\n",
    "c4[\"errEPv_cov\"] = edv.copy()\n",
    "c4[\"errF_vert_cov\"] = esum(edu, edv)\n",
    "\n",
    "c4[\"errF_total\"] = esum(c4[\"errF_vert\"], c4[\"errF_horiz\"])\n",
    "c4[\"errF_total_cov\"] = esum(c4[\"errF_vert_cov\"], c4[\"errF_horiz_cov\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the interpolated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "########################## SAVE CORRECTED FILES ##########################"
   },
   "outputs": [],
   "source": [
    "io.savemat(os.path.join(data_out, \"C_alt.mat\"), c4)\n",
    "io.savemat(os.path.join(data_out, \"C_altw.mat\"), c4w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ADCP\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADCP Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "########################## PROCESS ADCP DATA #############################"
   },
   "outputs": [],
   "source": [
    "print(\"ADCP PROCESSING\")\n",
    "tf = np.array([16.0, 2.0])  # band pass filter cut off hours\n",
    "tc_hrs = 40.0  # Low pass cut off (hours)\n",
    "dt = 0.5  # Data sample period hr\n",
    "\n",
    "print(\"Loading ADCP data from file.\")\n",
    "file = os.path.expanduser(os.path.join(data_in, \"ladcp_data.mat\"))\n",
    "adcp = utils.loadmat(file)[\"ladcp2\"]\n",
    "\n",
    "print(\"Removing all NaN rows.\")\n",
    "varl = [\"u\", \"v\", \"z\"]\n",
    "for var in varl:  # Get rid of the all nan row.\n",
    "    adcp[var] = adcp.pop(var)[:-1, :]\n",
    "\n",
    "print(\"Calculating vertical shear.\")\n",
    "z = adcp[\"z\"]\n",
    "dudz = np.diff(adcp[\"u\"], axis=0) / np.diff(z, axis=0)\n",
    "dvdz = np.diff(adcp[\"v\"], axis=0) / np.diff(z, axis=0)\n",
    "nans = np.isnan(dudz) | np.isnan(dvdz)\n",
    "dudz[nans] = np.nan\n",
    "dvdz[nans] = np.nan\n",
    "\n",
    "adcp[\"zm\"] = utils.mid(z, axis=0)\n",
    "adcp[\"dudz\"] = dudz\n",
    "adcp[\"dvdz\"] = dvdz\n",
    "\n",
    "# Low pass filter data.\n",
    "print(\"Low pass filtering at {:1.0f} hrs.\".format(tc_hrs))\n",
    "varl = [\"u\", \"v\", \"dudz\", \"dvdz\"]\n",
    "for var in varl:\n",
    "    data = adcp[var]\n",
    "    nans = np.isnan(data)\n",
    "    adcp[var + \"_m\"] = np.nanmean(data, axis=0)\n",
    "    datalo = utils.butter_filter(\n",
    "        utils.interp_nans(adcp[\"dates\"], data, axis=1), 1 / tc_hrs, 1 / dt, btype=\"low\"\n",
    "    )\n",
    "\n",
    "    # Then put nans back...\n",
    "    if nans.any():\n",
    "        datalo[nans] = np.nan\n",
    "\n",
    "    namelo = var + \"_lo\"\n",
    "    adcp[namelo] = datalo\n",
    "    namehi = var + \"_hi\"\n",
    "    adcp[namehi] = adcp[var] - adcp[namelo]\n",
    "\n",
    "# Band pass filter the data.\n",
    "print(\"Band pass filtering between {:1.0f} and {:1.0f} hrs.\".format(*tf))\n",
    "varl = [\"u\", \"v\", \"dudz\", \"dvdz\"]\n",
    "for var in varl:\n",
    "    data = adcp[var]\n",
    "    nans = np.isnan(data)\n",
    "    databp = utils.butter_filter(\n",
    "        utils.interp_nans(adcp[\"dates\"], data, axis=1), 1 / tf, 1 / dt, btype=\"band\"\n",
    "    )\n",
    "\n",
    "    # Then put nans back...\n",
    "    if nans.any():\n",
    "        databp[nans] = np.nan\n",
    "\n",
    "    namebp = var + \"_bp\"\n",
    "    adcp[namebp] = databp\n",
    "\n",
    "io.savemat(os.path.join(data_out, \"ADCP.mat\"), adcp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"VMP\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VMP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VMP PROCESSING\")\n",
    "vmp = utils.loadmat(os.path.join(data_in, \"jc054_vmp_cleaned.mat\"))[\"d\"]\n",
    "\n",
    "box = np.array([[-58.0, -58.0, -57.7, -57.7], [-56.15, -55.9, -55.9, -56.15]]).T\n",
    "p = path.Path(box)\n",
    "in_box = p.contains_points(np.vstack((vmp[\"startlon\"], vmp[\"startlat\"])).T)\n",
    "idxs = np.argwhere(in_box).squeeze()\n",
    "Np = len(idxs)\n",
    "\n",
    "print(\"Isolate profiles in match around mooring.\")\n",
    "for var in vmp:\n",
    "    ndim = np.ndim(vmp[var])\n",
    "    if ndim == 2:\n",
    "        vmp[var] = vmp[var][:, idxs]\n",
    "    if ndim == 1 and vmp[var].size == 36:\n",
    "        vmp[var] = vmp[var][idxs]\n",
    "\n",
    "print(\"Rename variables.\")\n",
    "vmp[\"P\"] = vmp.pop(\"press\")\n",
    "vmp[\"T\"] = vmp.pop(\"temp\")\n",
    "vmp[\"S\"] = vmp.pop(\"salin\")\n",
    "\n",
    "print(\"Deal with profiles where P[0] != 1.\")\n",
    "P_ = np.arange(1.0, 10000.0)\n",
    "i0o = np.zeros((Np), dtype=int)\n",
    "i1o = np.zeros((Np), dtype=int)\n",
    "i0n = np.zeros((Np), dtype=int)\n",
    "i1n = np.zeros((Np), dtype=int)\n",
    "pmax = 0.0\n",
    "for i in range(Np):\n",
    "    nans = np.isnan(vmp[\"eps\"][:, i])\n",
    "    i0o[i] = i0 = np.where(~nans)[0][0]\n",
    "    i1o[i] = i1 = np.where(~nans)[0][-1]\n",
    "    P0 = vmp[\"P\"][i0, i]\n",
    "    P1 = vmp[\"P\"][i1, i]\n",
    "    i0n[i] = np.searchsorted(P_, P0)\n",
    "    i1n[i] = np.searchsorted(P_, P1)\n",
    "    pmax = max(P1, pmax)\n",
    "\n",
    "P = np.tile(np.arange(1.0, pmax + 2)[:, np.newaxis], (1, len(idxs)))\n",
    "eps = np.full_like(P, np.nan)\n",
    "chi = np.full_like(P, np.nan)\n",
    "T = np.full_like(P, np.nan)\n",
    "S = np.full_like(P, np.nan)\n",
    "\n",
    "for i in range(Np):\n",
    "    eps[i0n[i] : i1n[i] + 1, i] = vmp[\"eps\"][i0o[i] : i1o[i] + 1, i]\n",
    "    chi[i0n[i] : i1n[i] + 1, i] = vmp[\"chi\"][i0o[i] : i1o[i] + 1, i]\n",
    "    T[i0n[i] : i1n[i] + 1, i] = vmp[\"T\"][i0o[i] : i1o[i] + 1, i]\n",
    "    S[i0n[i] : i1n[i] + 1, i] = vmp[\"S\"][i0o[i] : i1o[i] + 1, i]\n",
    "\n",
    "vmp[\"P\"] = P\n",
    "vmp[\"eps\"] = eps\n",
    "vmp[\"chi\"] = chi\n",
    "vmp[\"T\"] = T\n",
    "vmp[\"S\"] = S\n",
    "vmp[\"z\"] = gsw.z_from_p(vmp[\"P\"], vmp[\"startlat\"])\n",
    "\n",
    "print(\"Calculate neutral density.\")\n",
    "# Compute potential temperature using the 1983 UNESCO EOS.\n",
    "vmp[\"PT0\"] = seawater.ptmp(vmp[\"S\"], vmp[\"T\"], vmp[\"P\"])\n",
    "# Flatten variables for analysis.\n",
    "lons = np.ones_like(P) * vmp[\"startlon\"]\n",
    "lats = np.ones_like(P) * vmp[\"startlat\"]\n",
    "S_ = vmp[\"S\"].flatten()\n",
    "T_ = vmp[\"PT0\"].flatten()\n",
    "P_ = vmp[\"P\"].flatten()\n",
    "LO_ = lons.flatten()\n",
    "LA_ = lats.flatten()\n",
    "gamman = gamma_GP_from_SP_pt(S_, T_, P_, LO_, LA_)\n",
    "vmp[\"gamman\"] = np.reshape(gamman, vmp[\"P\"].shape) + 1000.0\n",
    "\n",
    "io.savemat(os.path.join(data_out, \"VMP.mat\"), vmp)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python2",
   "formats": "py:percent,ipynb",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "dimes-eiw",
   "language": "python",
   "name": "dimes-eiw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
